kubectl scale  --replicas=2 deployment/demoservice 

// Setup to-do
update env params for demo-service

add plaintext for kafka
add security tcp all for kafka

allow all tcp and all ports for redis

// Get inside pod
kubectl exec -it <pod-name> -- sh

// Check connection from node group ec2 in eks to other clusters like kafka and redis
nc -zv <b-2.kafka.tzqvpp.c22.kafka.us-east-1.amazonaws.com / public endpoint> <9092 / port-number>
nc -zv b-2.kafka.tzqvpp.c22.kafka.us-east-1.amazonaws.com 9094

// Find node group id
aws elasticache describe-replication-groups

---------------------------------------------------------------------------------------------------------------
kafka.m5.large

//Gatling run command inside the script gatlingChaos
./gradlew gatlingRun -DNO_OF_USERS=50 -DTPS1=10 -DDURATION=180]

CLOUDBURNER-SUPPORT@publicissapient.com

-----------------------------------------------------------------------------------------------------------------

//create main.tf and providers.tf files and configure the aws client with access key and secret key
// Change the vpc id's and subnet id's of .tf files
terraform init 
terraform plan
terraform apply

-----------------------------------------------------------------------------------------------------------
//Get the ClusterIP of your CoreDNS service:
kubectl get service kube-dns -n kube-system

//Verify that then DNS endpoints are exposed and pointing to CoreDNS pods
kubectl -n kube-system get endpoints kube-dns
//kube-dns   192.168.2.218:53,192.168.3.117:53,192.168.2.218:53 + 1 more...   90d

//Get the kube-proxy logs
kubectl logs -n kube-system --selector 'k8s-app=kube-proxy'

kubectl exec -it your-pod-name -- sh

// To verify that the cluster IP of the kube-dns service is in your pod's /etc/resolv.conf, run the following command in the shell inside of the pod:
cat /etc/resolv.conf

//To verify that your pod can use the default ClusterIP to resolve an internal domain, run the following command in the shell inside the pod
nslookup kubernetes 10.100.0.10

//To verify that your pod can use the default ClusterIP to resolve an external domain, run the following command in the shell inside the pod
nslookup amazon.com 10.100.0.10

//Enable the debug log of CoreDNS pods and add the log plugin to the CoreDNS ConfigMap
kubectl -n kube-system edit configmap coredns

----------------------------------------------------------------------------

//Sample IAM role
arn:aws:iam::669989449580:role/eksClusterRole2
arn:aws:iam::669989449580:role/eksWorkerRole2

//Describe configmap
kubectl describe configmap -n kube-system aws-auth

aws eks describe-cluster --name c2

//Get Role configuration
eksctl get iamidentitymapping --cluster c2 --region=us-east-1

//Config aws with newly created eks cluster 
aws eks --region us-east-1 update-kubeconfig --name demo

-------------------------------------------------------------------------------------------------------------------------------------------

//Create sample nginx deployment with 3 replicas 
kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml

kubectl get pods -l app=nginx

kubectl describe pod <pod-name>

kubectl delete deployment nginx-deployment

mkubectl get pods --selector=app=nginx

-------------------------------------------------------------------------------------------------------------------------------------------

//Install litmus chaos with kubectl
kubectl apply -f https://litmuschaos.github.io/litmus/2.0.0/litmus-2.0.0.yaml

kubectl apply -f https://raw.githubusercontent.com/litmuschaos/litmus/2.10.0/mkdocs/docs/2.10.0/litmus-2.10.0.yaml

//in namespace
kubectl apply -f https://raw.githubusercontent.com/litmuschaos/litmus/master/litmus-portal/manifests/cluster-k8s-manifest.yml

kubectl get pods -n litmus

kubectl get svc -n litmus

//patch loadbalancer
kubectl patch svc litmusportal-frontend-service -p '{"spec": {"type": "LoadBalancer"}}' -n litmus

//Troubleshooting
kubectl logs <name> -n litmus

//pod fix
kubectl get configmap -n litmus
kubectl edit configmap agent-config -n litmus
kubectl get pods -n litmus -o wide
//change server ip in notepad with node ip ( ip-<>-internal ) of server by kubectl get pods -n litmus -o wide, port would be same so no change mp

------------------------------------------------------------------------------------------------------------------------------------------

//delete everything
kubectl delete all --all
kubectl delete all --all -n litmus

kubectl get nodes -o yaml | grep pods 
//The command shows the pod count, that can be allocated to each nodes

kubectl get pods --all-namespaces | grep Running | wc -l
//How many are running?
 
kubectl describe node <node-name> | grep -i capacity -A 13
//Max capacity of pods 

//Calculate max no. of pods
curl -o max-pods-calculator.sh https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/files/max-pods-calculator.sh
chmod +x max-pods-calculator.sh
./max-pods-calculator.sh --instance-type m5.large --cni-version 1.9.0-eksbuild.1

-------------------------------------------------------------------------------------------------------------------------------------------

//install plugins on jenkins and add Credentials for dockerhub 

add this text while creating Credentials for Kubernetes Cluster
//sudo cat ~/.kube/config

//set a clusterrole as cluster-admin (one time only on ec2 instance)
kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous

-----------------------------------------------------------------------------------------------------------------
//custom chaos hub , private
URL : https://pscode.lioncloud.net/ransingh7/custom-chaos-hub.git
branch : main
secret access key : JszjxUssLCDXgKxqUJy1